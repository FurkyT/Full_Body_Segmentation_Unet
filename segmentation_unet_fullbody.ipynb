{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FurkyT/Full_Body_Segmentation_Unet/blob/main/segmentation_unet_fullbody.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvR3aIYyQMjv"
      },
      "source": [
        "**PÄ°P**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8N9xuoyQP42"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFtiFfAHO0H_"
      },
      "source": [
        "**Drive Connect For Download The .py Files To The Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Kma6qdO5AO",
        "outputId": "2d53cd02-018a-447b-dd18-e7b8345fdea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/ParcelDelineation/models\")\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/ParcelDelineation/utils\")\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/Segment_Model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzJlBrn4QnC"
      },
      "source": [
        "**Connectin Kaggle For Download The Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avgl15Sy4SRi"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600  ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download tapakah68/segmentation-full-body-mads-dataset\n",
        "!unzip segmentation-full-body-mads-dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkuWdFU6OhB"
      },
      "source": [
        "**Train Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmOV-DaU6Qg8"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image\n",
        "import segmentation_models as sm\n",
        "from segmentation_models import Unet\n",
        "from tensorflow.keras.applications import resnet50, densenet, mobilenet_v2\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.layers import Reshape, Concatenate, Conv2D, Conv2DTranspose, Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from keras.models import load_model\n",
        "import keras.losses #import cosine_proximity\n",
        "from keras import regularizers\n",
        "from PIL import Image\n",
        "from random import randint\n",
        "import unet\n",
        "import unet_dilated\n",
        "from data_loader_utils import batch_generator, batch_generator_DG\n",
        "import metrics\n",
        "from metrics import f1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import math\n",
        "import warnings\n",
        "import keras.backend as K\n",
        "import keras\n",
        "import pdb\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "sm.set_framework('tf.keras')\n",
        "sm.framework()\n",
        "\n",
        "assert len(sys.argv)==3\n",
        "prefix = sys.argv[1]\n",
        "epoch = 10\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "warnings.simplefilter('ignore', Image.DecompressionBombWarning)\n",
        "\n",
        "BACKBONE = 'resnet34'\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "def learning_rate_scheduler(epoch):\n",
        "    lr = 1e-4\n",
        "    '''\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 150:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    '''\n",
        "    print(\"Set Learning Rate : {}\".format(lr))\n",
        "    return lr\n",
        "\n",
        "\n",
        "#Set the variables here for training the model \n",
        "is_fill = False\n",
        "is_stacked = False\n",
        "is_imageNet = True\n",
        "is_dilated = False # dilated models are only for non-pretrained models \n",
        "\n",
        "num_channels = 3\n",
        "if is_stacked:\n",
        "    num_channels = 9\n",
        "\n",
        "input_shape = (640, 640, num_channels)\n",
        "batch_size = 12\n",
        "base_dir = '/content/'\n",
        "train_file = prefix + 'demoitrain.csv'\n",
        "val_file = prefix + 'demoival.csv'\n",
        "filepath = prefix + '-unet-segment.hdf5'\n",
        "csv_log_file = prefix + '_log_unet_segment.csv'\n",
        "\n",
        "#Loads training and validation data frame\n",
        "#Dataframe contains the paths of the images\n",
        "train_df = pd.read_csv(base_dir + train_file)\n",
        "val_df = pd.read_csv(base_dir + val_file)\n",
        "\n",
        "model = None \n",
        "\n",
        "if is_dilated:\n",
        "    model = unet_dilated(input_size = input_shape)\n",
        "elif is_imageNet:\n",
        "    model_unet = Unet(BACKBONE, encoder_weights='imagenet')\n",
        "    if is_stacked: \n",
        "        new_model = keras.models.Sequential()\n",
        "        new_model.add(Conv2D(3, (1,1), padding='same', activation='relu', input_shape=input_shape))\n",
        "        new_model.add(model_unet)\n",
        "        model = new_model\n",
        "    else:\n",
        "        model = model_unet\n",
        "else:\n",
        "    model = unet(input_size=input_shape)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=learning_rate_scheduler(0)),\n",
        "              metrics=['acc', f1])\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='f1', verbose=1, save_best_only=True, mode='max')\n",
        "csv_logger = CSVLogger(csv_log_file, append=True, separator=';')\n",
        "callbacks_list = [checkpoint, csv_logger]\n",
        "\n",
        "model.fit_generator(batch_generator_DG(train_df, batch_size, is_imageNet), steps_per_epoch=round((len(train_df))/batch_size),\n",
        "        epochs=int(epoch), validation_data=batch_generator_DG(val_df, batch_size, is_imageNet), validation_steps=round((len(val_df))/batch_size),callbacks=callbacks_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict Cell For Test Data**"
      ],
      "metadata": {
        "id": "AyuprI1pkXFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import segmentation_models as sm\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.applications import resnet50, densenet, mobilenet_v2\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.layers import Reshape, Concatenate, Conv2D, Conv2DTranspose, Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from keras.models import load_model\n",
        "import keras.losses #import cosine_proximity\n",
        "from keras import regularizers\n",
        "from PIL import Image\n",
        "import unet_dilated\n",
        "from segmentation_models import Unet\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import math\n",
        "import warnings\n",
        "import keras.backend as K\n",
        "import pdb\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "import keras.losses\n",
        "from matplotlib import pyplot as plt\n",
        "from metrics import get_metrics, f1, dice_coef_sim\n",
        "from data_loader_utils import batch_generator, batch_generator_DG, read_imgs_keraspp\n",
        "\n",
        "BACKBONE = 'resnet34'\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "def learning_rate_scheduler(epoch):\n",
        "    lr = 1e-4\n",
        "    '''\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 150:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    '''\n",
        "    print(\"Set Learning Rate : {}\".format(lr))\n",
        "    return lr\n",
        "\n",
        "#Set the filepaths here for laoding in the file \n",
        "is_fill = False\n",
        "is_stacked = False\n",
        "is_imageNet = True\n",
        "is_dilated = False\n",
        "image_type = 'sentinel' \n",
        "\n",
        "\n",
        "batch_size = 1\n",
        "num_channels = 3\n",
        "if is_stacked:\n",
        "    num_channels = 9\n",
        "\n",
        "\n",
        "base_dir = '/content/'\n",
        "val_file = '-fdemoival' \n",
        "filepath= '/content/drive/MyDrive/Segment_Model/-f-unet-segment' \n",
        "csv_log_file = '/content/drive/MyDrive/Segment_Model/-f_log_unet_segment.csv'\n",
        "\n",
        "sub_fill = ''\n",
        "if is_fill:\n",
        "    sub_fill = '_fill'\n",
        "\n",
        "#Modify file path depending on fill/boundary task\n",
        "val_file = val_file + sub_fill + '.csv'\n",
        "# File path for the model\n",
        "filepath = filepath + sub_fill + '.hdf5'\n",
        "# Csv log file\n",
        "csv_log_file = csv_log_file + sub_fill + '.csv'\n",
        "\n",
        "#Loads validation data frame\n",
        "test_df = pd.read_csv(base_dir + val_file)\n",
        "\n",
        "pred_file= \"ci_predictions.npy\" \n",
        "dependencies = {'f1':f1}\n",
        "\n",
        "model = load_model(filepath, custom_objects=dependencies)\n",
        "\n",
        "history = model.predict_generator(batch_generator_DG(test_df, batch_size, is_imageNet), steps = round(len(test_df)/batch_size))\n",
        "history = history.squeeze()\n",
        "np.save(pred_file, history)\n",
        "\n",
        "predictions = np.load(pred_file) \n",
        "x_true, y_true = read_imgs_keraspp(test_df)\n",
        "y_true = y_true.flatten()\n",
        "y_pred = predictions.flatten()\n",
        "\n",
        "get_metrics(y_true, y_pred, binarized=False)\n"
      ],
      "metadata": {
        "id": "dvSK22AZkYfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5FaO_GL9BbI"
      },
      "source": [
        "**Making CSV For Data Paths and Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyV5WJQU9DhV"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def write_to_csv(pairs, file_name_split_csv, header =['image','mask']):\n",
        "  with open(file_name_split_csv, 'w') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    csv_writer.writerow(header)\n",
        "    for pair in pairs:\n",
        "      row0 = pair[0]\n",
        "      row1 = pair[1]\n",
        "      rows = [row0, row1]\n",
        "      csv_writer.writerow(rows)\n",
        "\n",
        "base = '/content/segmentation_full_body_mads_dataset_1192_img/'\n",
        "images = 'images/'\n",
        "masks = 'masks/'\n",
        "\n",
        "\n",
        "ipath = base + images\n",
        "mpath = base + masks\n",
        "\n",
        "df = pd.read_csv(\"images_t.txt\")\n",
        "df.columns = [\"path\"]\n",
        "df2 = pd.read_csv(\"masks_t.txt\")\n",
        "df2.columns = [\"path\"]\n",
        "\n",
        "\n",
        "imfiles = []\n",
        "mafiles = []\n",
        "\n",
        "for i in df.path:\n",
        "  imfiles.append(\"/content/segmentation_full_body_mads_dataset_1192_img/images/\" + i)\n",
        "for i in df2.path:\n",
        "  mafiles.append(\"/content/segmentation_full_body_mads_dataset_1192_img/masks/\" + i)\n",
        "\n",
        "\n",
        "\n",
        "impairs = []\n",
        "\n",
        "\n",
        "for f in imfiles:\n",
        "    for f_ in mafiles:\n",
        "        if f.split('/')[-1] == f_.split('/')[-1]:\n",
        "            impairs.append((f, f_))\n",
        "   \n",
        "\n",
        "np.random.shuffle(impairs)\n",
        "\n",
        "\n",
        "split = len(impairs)//10*9\n",
        "imtrain = impairs[:split]\n",
        "imval = impairs[split:]\n",
        "\n",
        "\n",
        "print('Ä°mage train size: {}'.format(len(imtrain)))\n",
        "print('Ä°mage val size: {}'.format(len(imval)))\n",
        "\n",
        "\n",
        "imtrain_csv = '-fdemoitrain.csv'\n",
        "imval_csv = '-fdemoival.csv'\n",
        "\n",
        "\n",
        "write_to_csv(imtrain, imtrain_csv)\n",
        "write_to_csv(imval, imval_csv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_qySGIBeCaJ"
      },
      "source": [
        "**Saving The Paths Into a .txt File For Read With Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQrDa6jaeFdA"
      },
      "outputs": [],
      "source": [
        "! ls /content/segmentation_full_body_mads_dataset_1192_img/images > /content/images_t.txt\n",
        "! ls /content/segmentation_full_body_mads_dataset_1192_img/masks > /content/masks_t.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Displaying The Test Prediction Data**"
      ],
      "metadata": {
        "id": "vWc9KKVbmS2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from PIL import Image as im\n",
        "\n",
        "images = numpy.load('ci_predictions.npy')\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(images[0], interpolation='nearest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zk3SEPHJmWKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from PIL import Image as im\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "ima = []\n",
        "for i in range(len(images)):\n",
        "  img = images[i]\n",
        "  img[img<0.5] = 0\n",
        "  img[img != 0] = 255\n",
        "  ima.append(img)\n",
        "\n",
        "#img = images[0]\n",
        "#img[img<0.5] = 0\n",
        "#img[img != 0] = 255\n",
        "cv2_imshow(ima[7])"
      ],
      "metadata": {
        "id": "vKxm7hAapwNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function For Giving A Random Image To The Model For Prediction**"
      ],
      "metadata": {
        "id": "g0CxhvJprqZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predict(test):\n",
        "  n_img = Image.open(test)\n",
        "  n_img = n_img.resize((640,640)).convert('RGB')\n",
        "  n_img = np.expand_dims(n_img, axis=0)\n",
        "  n_img = np.array(n_img)\n",
        "  load_model('/content/drive/MyDrive/Segment_Model/-f-unet-segment.hdf5', custom_objects=dependencies)\n",
        "  pred = model.predict(n_img,\n",
        "                batch_size=None,\n",
        "                verbose='auto',\n",
        "                steps=None,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=True)\n",
        "  return pred"
      ],
      "metadata": {
        "id": "6ZQwp-HirsB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_img = get_predict('/content/test0.png') # Image path for external data\n",
        "n_img = n_img.reshape(640,640,)\n",
        "n_img[n_img<0.5] = 0\n",
        "n_img[n_img != 0] = 255\n",
        "cv2_imshow(n_img)"
      ],
      "metadata": {
        "id": "Q4RnLGHrsF5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "segmentation_unet_fullbody.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGMXkIVH9J+1dwYK9UcMMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}